# -*- coding: utf-8 -*-
"""python logistic_regression_intrusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z0dloW2jG2bPD4XKJ9C8uKBoFsLs24of

# Load data
"""

!pip install ucimlrepo

# fetch dataset
from ucimlrepo import fetch_ucirepo, list_available_datasets
rt_iot2022 = fetch_ucirepo(id=942)

import pandas as pd
iot_data_features = rt_iot2022.data.features
iot_data_targets = rt_iot2022.data.targets

# Convert the extracted data to a pandas DataFrame
iot_data_features_df = pd.DataFrame(iot_data_features)
iot_data_targets_df = pd.DataFrame(iot_data_targets)

"""# Overview of data"""

# Check the initial features of the dataset
iot_data_features_df.head()

# Check the inital targets of the dataset
iot_data_targets_df.head()

# View more information about the features of the dataset
iot_data_features_df.info()

# Dscribe the data features
iot_data_features_df.describe()

"""# Visualize Data"""

# Visualize the data
import matplotlib.pyplot as plt
import seaborn as sns

# Create subplots with specified figure size
fig, ax = plt.subplots(figsize=(15, 6))

# Create count plot
sns.countplot(data=iot_data_targets_df, x='Attack_type', order=iot_data_targets_df['Attack_type'].value_counts().index, ax=ax,color='navy')

# Rotate x-axis labels
plt.xticks(rotation=45)

# Add header text
plt.text(x=0.5, y=1.05, s='Distribution of Attack Types', fontsize=16, ha='center', transform=ax.transAxes)

# Show the plot
plt.show()

# Create subplots with specified figure size
fig, ax = plt.subplots(figsize=(6, 4))

# Create count plot
sns.countplot(data=iot_data_features_df, x='proto', order=iot_data_features_df['proto'].value_counts().index, ax=ax, color='tomato')

# Rotate x-axis labels
plt.xticks(rotation=45)

# Add header text
plt.text(x=0.5, y=1.05, s='Distribution of Protocol used in communication', fontsize=16, ha='center', transform=ax.transAxes)

# Show the plot
plt.show()

# Create subplots with specified figure size
fig, ax = plt.subplots(figsize=(6, 4))

# Create count plot
sns.countplot(data=iot_data_features_df, x='service', order=iot_data_features_df['service'].value_counts().index, ax=ax,color='teal')

# Rotate x-axis labels
plt.xticks(rotation=45)

# Add header text
plt.text(x=0.5, y=1.05, s='Distribution of services related to communication', fontsize=16, ha='center', transform=ax.transAxes)

# Show the plot
plt.show()

# concatinate the data frames
iot_data_df = pd.concat([iot_data_features_df, iot_data_targets_df], axis=1)

# Group by Attack Type and sum Flow Duration
attackType_flowDuration_fig = iot_data_df.groupby('Attack_type')['flow_duration'].sum()

# Plot the data
fig, ax = plt.subplots(figsize=(20, 4))
attackType_flowDuration_fig.plot(ax=ax,color='navy')

# Add a header
ax.set_title('Total Flow Duration by Attack Type')

# Show the plot
plt.show()

# Setting up the figure with multiple subplots for different features
fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6, 18))

# List of key features to plot
key_features = ['flow_FIN_flag_count', 'fwd_URG_flag_count', 'fwd_pkts_payload.min']

# Plotting distributions for selected features
for i, feature in enumerate(key_features):
    sns.histplot(data=iot_data_df, x=feature, hue='Attack_type', multiple='stack', ax=axes[i])
    axes[i].set_title(f'Distribution of {feature}')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# Group by Attack Type and sum packet counts
attackType_fwdBwdPoints_fig = iot_data_df.groupby('Attack_type')[['fwd_pkts_tot', 'bwd_pkts_tot', 'fwd_data_pkts_tot', 'bwd_data_pkts_tot']].sum()

# Plot the data
plt.figure(figsize=(20, 3))
sns.lineplot(attackType_fwdBwdPoints_fig,  palette='rocket',linewidth=2.5,  markers=True)

# Add title and rotate x-axis labels
plt.title('Total Packet Counts by Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# DOS_SYN_HPING - having much higher fwd_pkts_tot and also followed up with bwd_pkts_tot this creates an anomoly detection.
#Thing_speak - also have higher fwd_pkts_tot and our bwd_pkts_tot as whenever fw_pkts_tot moves forward our bwd_pkts_tot also have an impact on that

# Group by Attack Type and sum packet counts
attackType_fwdBwdPkts_fig = iot_data_df.groupby('Attack_type')[['fwd_pkts_per_sec','bwd_pkts_per_sec','flow_pkts_per_sec','down_up_ratio']].sum()

# Plot the data
plt.figure(figsize=(20, 5))
sns.lineplot(attackType_fwdBwdPkts_fig,  palette='mako',linewidth=2.5)

# Add title and rotate x-axis labels
plt.title(' Packet flow speed Counts by Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# From this graph we can see that our flow_pkts_per_sec has been increasing while having DOS_SYN_Hping

# Group by Attack Type and sum packet header counts
attackType_fwdBwdPkts_fig = iot_data_df.groupby('Attack_type')[['fwd_header_size_tot','fwd_header_size_min','fwd_header_size_max','bwd_header_size_tot','bwd_header_size_min','bwd_header_size_max']].sum()

# Plot the data
plt.figure(figsize=(20, 5))
sns.lineplot(attackType_fwdBwdPkts_fig,  palette='tab10',linewidth=2.5)

# Add title and rotate x-axis labels
plt.title(' Packet header counts  by Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# While in above figure we can see that the features we have taken to view anomaly dectection have much higher impact on hyping and Slowlories + we can see those thing also going much higher in Thing_Speak

# Group by Attack Type and no of packets in the flow with flags
attackType_flowFlags_fig = iot_data_df.groupby('Attack_type')[['flow_FIN_flag_count','flow_SYN_flag_count','flow_RST_flag_count','flow_ACK_flag_count','flow_CWR_flag_count','flow_ECE_flag_count']].sum()

# Plot the data
plt.figure(figsize=(20, 5))
sns.lineplot(attackType_flowFlags_fig,  palette='Paired',linewidth=2.5)

# Add title and rotate x-axis labels
plt.title('Packet count in the flow with the flags by Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# In this we can see that if the flow_syn_count is triggered it also trigger flow_ACK_flag_count + flow_RST_flag_count
# but this do no go with Thing_speak as before all the features has been categories this system as anomaly so this not as much triggring comparing to other attacks

# Group by Attack Type and no of packets in the forward/backward flow with flags
attackType_fwbwflowFlags_fig = iot_data_df.groupby('Attack_type')[['fwd_PSH_flag_count','bwd_PSH_flag_count','fwd_URG_flag_count','bwd_URG_flag_count']].sum()

# Plot the data
plt.figure(figsize=(20, 5))
sns.lineplot(attackType_fwbwflowFlags_fig,  palette='rocket',linewidth=2.5)

# Add title and rotate x-axis labels
plt.title('Packet count in the forward/backward flow with flags along with Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# Here we can counter how this bwd_psh_flag_count that data should be pushed as soon as possible data is recieved and send it to first layer.
# while the URG flag indicates that certain data requires immediate attention - there is only one point where we can view this flag NMAP_XMAS_TREE_SCAN

# Group by Attack Type and Minimum & Maximum size of data payload in packets
attackType_MinMaxPayload_fig = iot_data_df.groupby('Attack_type')[['fwd_pkts_payload.min','fwd_pkts_payload.max','bwd_pkts_payload.min','bwd_pkts_payload.max']].sum()

# Plot the data
plt.figure(figsize=(20, 5))
sns.lineplot(attackType_MinMaxPayload_fig,  palette='rocket',linewidth=2.5)

# Add title and rotate x-axis labels
plt.title('Minimum & Maximum size of data payload in packets along with Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# While having DOS_SYN_Hping our fwd_pkts_payload.max goes very much higher
# as dealing with Thing_Speak our bwd_pkts_payload.max goes very much higher

# Group by Attack Type and Total size of data payload in forwarded/backward  packets
attackType_TotFwBwPayload_fig = iot_data_df.groupby('Attack_type')[['fwd_pkts_payload.tot','bwd_pkts_payload.tot']].sum()

# Plot the data
plt.figure(figsize=(20, 5))
sns.lineplot(attackType_TotFwBwPayload_fig,  palette='Paired',linewidth=2.5)

# Add title and rotate x-axis labels
plt.title('Total size of data payload in forwarded/backward  packets along with Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# Anamoly pretty much detected in MQTT_Publish when we have bwd_payload vrey high and opposite goes to DOS_SYN_Hping as we can detect much anomaly when fwd_payload is much higher.

# Group by Attack Type and Average size of data payload  in forwarded/backward  packets
attackType_avgFwBwPayload_fig = iot_data_df.groupby('Attack_type')[['fwd_pkts_payload.avg','bwd_pkts_payload.avg']].sum()

# Plot the data
plt.figure(figsize=(20, 5))
sns.lineplot(attackType_avgFwBwPayload_fig,  palette='Paired',linewidth=2.5)

# Add title and rotate x-axis labels
plt.title('Average size of data payload  in forwarded/backward packets along with Attack Type')
plt.xticks(rotation=10)

# Show the plot
plt.tight_layout()
plt.show()

# DOS_SYN_Hping pretty much makes Avg goes much higher compare to other

"""# Convert the categorical columns to numeric"""

import pandas as pd

def rename_categories(df, target_column, category_mapping):
    """
    Rename categories in the target column of a DataFrame based on a mapping.

    Parameters:
    - df: DataFrame, the DataFrame containing the target column
    - target_column: str, the name of the target column
    - category_mapping: dict, a dictionary mapping current category names to new numerical labels

    Returns:
    - DataFrame, a new DataFrame with the target column renamed according to the mapping
    """
    df_copy = df.copy()  # Create a copy of the original DataFrame to avoid modifying it directly
    df_copy[target_column] = df_copy[target_column].map(category_mapping)
    return df_copy


# Mapping dictionary
category_mapping = {
    'MQTT_Publish': 0,
    'NMAP_TCP_scan':1,
    'Thing_Speak':0,
    'NMAP_XMAS_TREE_SCAN':1,
    'ARP_poisioning':1,
    'NMAP_UDP_SCAN':1,
    'Wipro_bulb':0,
    'DDOS_Slowloris':1,
    'NMAP_OS_DETECTION':1,
    'Metasploit_Brute_Force_SSH':1,
    'NMAP_FIN_SCAN':1,
    'DOS_SYN_Hping':1

}

# Apply the function to rename categories
targets_df = rename_categories(iot_data_targets_df, 'Attack_type', category_mapping)
print("\nDataFrame with Renamed Categories:")
print(targets_df .head())

# Get the categorical attributes of feature dataset
iot_data_features_df_cat = iot_data_features_df[["proto","service"]]

# Get only the numerical attributes of the feature dataset
iot_data_features_df_num = iot_data_features_df.drop(["proto","service"],axis=1)

# Convert the categorical values in the feature  data to numbers

from sklearn.preprocessing import OrdinalEncoder

# Create an instance of the ordianl encoder
ordinal_encoder = OrdinalEncoder()

num_feature_proto_categories = iot_data_features_df['proto'].nunique()
print("Number of unique categories in the proto column :", num_feature_proto_categories)

num_feature_service_categories = iot_data_features_df['service'].nunique()
print("Number of unique categories in the service column :", num_feature_service_categories)

iot_data_features_encoded = ordinal_encoder.fit_transform(iot_data_features_df_cat)
print(iot_data_features_encoded[:10])
ordinal_encoder.categories_

# Convert  NumPy array into a pandas DataFrame and new numerical columns names proto_num, service_num
iot_data_features_encoded_df = pd.DataFrame(iot_data_features_encoded, columns=['proto','service'])
iot_data_features_encoded_df.head()

# Concat the numerical dataframes into a one
features_df = pd.concat([iot_data_features_df_num,iot_data_features_encoded_df], axis=1)
features_df.head()

"""# Split the data to train and test"""

# Create a Test Set

from sklearn.model_selection import train_test_split
features_train, features_test, targets_train, targets_test = train_test_split(features_df, targets_df, test_size=0.2, random_state=42)

print("Size of features of the Training Set: ", len(features_train))
print("Size of targets of the Training Set: ", len(targets_train))
print()
print("Size of features of the Test Set: ", len(features_test))
print("Size of targets of the Test Set: ", len(targets_test))

"""# Scale the Features"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features_train_scaled = scaler.fit_transform(features_train)
features_test_scaled = scaler.transform(features_test)

"""#  Logistic Regression with No Regularization"""

targets_train = targets_train['Attack_type'].to_numpy()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Train Logistic Regression with no regularization, solver = saga
log_reg_none = LogisticRegression(penalty=None, solver='saga', max_iter=1000,tol=1e-3)

log_reg_none.fit(features_train_scaled, targets_train)

# Predict on the training data
targets_train_pred = log_reg_none.predict(features_train_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Training Data):\n", classification_report(targets_train, targets_train_pred))

# Predict on the Test data
targets_test_pred = log_reg_none.predict(features_test_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Test Data):\n", classification_report(targets_test, targets_test_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Train Logistic Regression with no regularization solver = lbfgs
log_reg_none_s1 = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000,tol=1e-3)

log_reg_none_s1.fit(features_train_scaled, targets_train)

# Predict on the training data
targets_train_pred = log_reg_none_s1.predict(features_train_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Training Data):\n", classification_report(targets_train, targets_train_pred))

# Predict on the Test data
targets_test_pred = log_reg_none_s1.predict(features_test_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Test Data):\n", classification_report(targets_test, targets_test_pred))

"""# Logistic Regression with L2 regularization (ridge)

"""

# Train Logistic Regression with L2 regularization
log_reg_l2 = LogisticRegression(penalty='l2', solver='saga', max_iter=1000,tol=1e-3)
log_reg_l2.fit(features_train_scaled, targets_train)

# Predict on the training data
targets_train_pred = log_reg_l2.predict(features_train_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Training Data):\n", classification_report(targets_train, targets_train_pred))

# Predict on the Test data
targets_test_pred = log_reg_l2.predict(features_test_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Test Data):\n", classification_report(targets_test, targets_test_pred))

"""# Logistic Regression with L1-L2 regularization (elastic-net)"""

# Train Logistic Regression with L1 regularization
log_reg_elastic = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)
log_reg_elastic.fit(features_train_scaled, targets_train)

# Predict on the training data
targets_train_pred = log_reg_elastic.predict(features_train_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Training Data):\n", classification_report(targets_train, targets_train_pred))

# Predict on the Test data
targets_test_pred = log_reg_elastic.predict(features_test_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Test Data):\n", classification_report(targets_test, targets_test_pred))

"""# Logistic Regression with L1 regularization (Lasso)

"""

log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)
log_reg_l1.fit(features_train_scaled, targets_train)

# Predict on the training data
targets_train_pred = log_reg_l1.predict(features_train_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Training Data):\n", classification_report(targets_train, targets_train_pred))

# Predict on the Test data
targets_test_pred = log_reg_l1.predict(features_test_scaled)

# Print the classification report for the training data predictions
print("Classification Report (Test Data):\n", classification_report(targets_test, targets_test_pred))

"""# PCA dimensionality reduction- No regularization



"""

from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score

# Apply PCA
pca = PCA(n_components=0.95)  # Retain 95% of the variance
features_train_scaled_pca = pca.fit_transform(features_train_scaled)

features_test_scaled_pca = pca.transform(features_test_scaled)

# Train Logistic Regression on the PCA-transformed data
log_reg = LogisticRegression(penalty=None, solver='saga', max_iter=1000,tol=1e-3)
log_reg.fit(features_train_scaled_pca, targets_train)
targets_pred = log_reg.predict(features_test_scaled_pca)

# Evaluate the model
accuracy = accuracy_score(targets_test, targets_pred)
print("Logistic Regression with No regularization- Accuracy with PCA:", accuracy)
print(classification_report(targets_test, targets_pred))

"""# PCA dimensionality reduction-L2 regularization (ridge)"""

# Apply PCA
pca_l2 = PCA(n_components=0.95)  # Retain 95% of the variance
features_train_scaled_pca_l2 = pca_l2.fit_transform(features_train_scaled)

features_test_scaled_pca_l2 = pca.transform(features_test_scaled)

# Train Logistic Regression on the PCA-transformed data
log_reg_l2= LogisticRegression(penalty="l2", solver='saga', max_iter=1000,tol=1e-3)
log_reg_l2.fit(features_train_scaled_pca_l2, targets_train)
targets_pred = log_reg.predict(features_test_scaled_pca_l2)

# Evaluate the model
accuracy = accuracy_score(targets_test, targets_pred)
print("Logistic Regression with L2 regularization- Accuracy with PCA:", accuracy)
print(classification_report(targets_test, targets_pred))

"""# PCA dimensionality reduction - L1-L2 regularization (elastic-net)"""

# Apply PCA
pca_l2_l1 = PCA(n_components=0.95)  # Retain 95% of the variance
features_train_scaled_pca_l2l1 = pca_l2_l1.fit_transform(features_train_scaled)

features_test_scaled_pca_l2l1 = pca.transform(features_test_scaled)

# Train Logistic Regression on the PCA-transformed data
log_reg_l2l1= LogisticRegression(penalty="elasticnet", solver='saga', max_iter=1000,tol=1e-3)
log_reg_l2l1.fit(features_train_scaled_pca_l2l1, targets_train)
targets_pred = log_reg.predict(features_test_scaled_pca_l2l1)